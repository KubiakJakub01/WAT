{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Policy Gradient w MuJoCo\n",
        "\n",
        "Poniższy kod przedstawia zastosowanie metody Policy Gradient w środowisku [Inverted Pendulum](https://www.gymlibrary.dev/environments/mujoco/inverted_pendulum/).\n"
      ],
      "metadata": {
        "id": "4lnTspDXi-pD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install glfw\n",
        "!pip install mujoco\n",
        "!pip install mujoco-py\n",
        "!pip install gym[mujoco]\n",
        "\n",
        "!pip install free-mujoco-py\n",
        "\n",
        "!apt-get install -y \\\n",
        "    libgl1-mesa-dev \\\n",
        "    libgl1-mesa-glx \\\n",
        "    libglew-dev \\\n",
        "    libosmesa6-dev \\\n",
        "    software-properties-common\n",
        "\n",
        "!apt-get install -y patchelf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9sriiGDLa3u6",
        "outputId": "9ff2a1d6-f120-4061-8a50-20594432aa11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: glfw in /usr/local/lib/python3.10/dist-packages (1.12.0)\n",
            "Requirement already satisfied: mujoco in /usr/local/lib/python3.10/dist-packages (2.2.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mujoco) (1.4.0)\n",
            "Requirement already satisfied: glfw in /usr/local/lib/python3.10/dist-packages (from mujoco) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mujoco) (1.26.4)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.10/dist-packages (from mujoco) (3.1.7)\n",
            "Requirement already satisfied: mujoco-py in /usr/local/lib/python3.10/dist-packages (2.1.2.14)\n",
            "Requirement already satisfied: glfw>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from mujoco-py) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.10/dist-packages (from mujoco-py) (1.26.4)\n",
            "Requirement already satisfied: Cython>=0.27.2 in /usr/local/lib/python3.10/dist-packages (from mujoco-py) (0.29.37)\n",
            "Requirement already satisfied: imageio>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from mujoco-py) (2.36.1)\n",
            "Requirement already satisfied: cffi>=1.10 in /usr/local/lib/python3.10/dist-packages (from mujoco-py) (1.17.1)\n",
            "Requirement already satisfied: fasteners~=0.15 in /usr/local/lib/python3.10/dist-packages (from mujoco-py) (0.15)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.10->mujoco-py) (2.22)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fasteners~=0.15->mujoco-py) (1.16.0)\n",
            "Requirement already satisfied: monotonic>=0.1 in /usr/local/lib/python3.10/dist-packages (from fasteners~=0.15->mujoco-py) (1.6)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio>=2.1.2->mujoco-py) (11.0.0)\n",
            "Requirement already satisfied: gym[mujoco] in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[mujoco]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[mujoco]) (3.1.0)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[mujoco]) (0.0.8)\n",
            "Requirement already satisfied: mujoco==2.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[mujoco]) (2.2.0)\n",
            "Requirement already satisfied: imageio>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from gym[mujoco]) (2.36.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mujoco==2.2.0->gym[mujoco]) (1.4.0)\n",
            "Requirement already satisfied: glfw in /usr/local/lib/python3.10/dist-packages (from mujoco==2.2.0->gym[mujoco]) (1.12.0)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.10/dist-packages (from mujoco==2.2.0->gym[mujoco]) (3.1.7)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio>=2.14.1->gym[mujoco]) (11.0.0)\n",
            "Requirement already satisfied: free-mujoco-py in /usr/local/lib/python3.10/dist-packages (2.1.6)\n",
            "Requirement already satisfied: Cython<0.30.0,>=0.29.24 in /usr/local/lib/python3.10/dist-packages (from free-mujoco-py) (0.29.37)\n",
            "Requirement already satisfied: cffi<2.0.0,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from free-mujoco-py) (1.17.1)\n",
            "Requirement already satisfied: fasteners==0.15 in /usr/local/lib/python3.10/dist-packages (from free-mujoco-py) (0.15)\n",
            "Requirement already satisfied: glfw<2.0.0,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from free-mujoco-py) (1.12.0)\n",
            "Requirement already satisfied: imageio<3.0.0,>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from free-mujoco-py) (2.36.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.21.3 in /usr/local/lib/python3.10/dist-packages (from free-mujoco-py) (1.26.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fasteners==0.15->free-mujoco-py) (1.16.0)\n",
            "Requirement already satisfied: monotonic>=0.1 in /usr/local/lib/python3.10/dist-packages (from fasteners==0.15->free-mujoco-py) (1.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi<2.0.0,>=1.15.0->free-mujoco-py) (2.22)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0.0,>=2.9.0->free-mujoco-py) (11.0.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libglew-dev is already the newest version (2.2.0-4).\n",
            "libgl1-mesa-dev is already the newest version (23.2.1-1ubuntu3.1~22.04.2).\n",
            "libosmesa6-dev is already the newest version (23.2.1-1ubuntu3.1~22.04.2).\n",
            "software-properties-common is already the newest version (0.99.22.9).\n",
            "libgl1-mesa-glx is already the newest version (23.0.4-0ubuntu1~22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  patchelf\n",
            "0 upgraded, 1 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 72.1 kB of archives.\n",
            "After this operation, 186 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 patchelf amd64 0.14.3-1 [72.1 kB]\n",
            "Fetched 72.1 kB in 0s (164 kB/s)\n",
            "Selecting previously unselected package patchelf.\n",
            "(Reading database ... 123772 files and directories currently installed.)\n",
            "Preparing to unpack .../patchelf_0.14.3-1_amd64.deb ...\n",
            "Unpacking patchelf (0.14.3-1) ...\n",
            "Setting up patchelf (0.14.3-1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import gym\n",
        "import os\n",
        "from general import get_logger, Progbar, export_plot\n",
        "from baseline_network import BaselineNetwork\n",
        "from network_utils import build_mlp, device, np2torch\n",
        "from policy import CategoricalPolicy, GaussianPolicy\n",
        "\n",
        "\n",
        "class PolicyGradient(object):\n",
        "    \"\"\"\n",
        "    Class for implementing a policy gradient algorithm\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env, config, seed, logger=None):\n",
        "        \"\"\"\n",
        "        Initialize Policy Gradient Class\n",
        "\n",
        "        Args:\n",
        "                env: an OpenAI Gym environment\n",
        "                config: class with hyperparameters\n",
        "                logger: logger instance from the logging module\n",
        "\n",
        "        You do not need to implement anything in this function. However,\n",
        "        you will need to use self.discrete, self.observation_dim,\n",
        "        self.action_dim, and self.lr in other methods.\n",
        "        \"\"\"\n",
        "        # directory for training outputs\n",
        "        if not os.path.exists(config.output_path):\n",
        "            os.makedirs(config.output_path)\n",
        "\n",
        "        # store hyperparameters\n",
        "        self.config = config\n",
        "        self.seed = seed\n",
        "\n",
        "        self.logger = logger\n",
        "        if logger is None:\n",
        "            self.logger = get_logger(config.log_path)\n",
        "        self.env = env\n",
        "        self.env.seed(self.seed)\n",
        "\n",
        "        # discrete vs continuous action space\n",
        "        self.discrete = isinstance(env.action_space, gym.spaces.Discrete)\n",
        "        self.observation_dim = self.env.observation_space.shape[0]\n",
        "        self.action_dim = (\n",
        "            self.env.action_space.n if self.discrete else self.env.action_space.shape[0]\n",
        "        )\n",
        "\n",
        "        self.lr = self.config.learning_rate\n",
        "\n",
        "        self.init_policy()\n",
        "\n",
        "    def init_policy(self):\n",
        "        network = build_mlp(self.observation_dim, self.action_dim, self.config.n_layers, self.config.layer_size)\n",
        "        self.policy = GaussianPolicy(network, self.action_dim)\n",
        "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=self.lr)\n",
        "\n",
        "    def init_averages(self):\n",
        "        \"\"\"\n",
        "        You don't have to change or use anything here.\n",
        "        \"\"\"\n",
        "        self.avg_reward = 0.0\n",
        "        self.max_reward = 0.0\n",
        "        self.std_reward = 0.0\n",
        "        self.eval_reward = 0.0\n",
        "\n",
        "    def update_averages(self, rewards, scores_eval):\n",
        "        \"\"\"\n",
        "        Update the averages.\n",
        "        You don't have to change or use anything here.\n",
        "\n",
        "        Args:\n",
        "            rewards: deque\n",
        "            scores_eval: list\n",
        "        \"\"\"\n",
        "        self.avg_reward = np.mean(rewards)\n",
        "        self.max_reward = np.max(rewards)\n",
        "        self.std_reward = np.sqrt(np.var(rewards) / len(rewards))\n",
        "\n",
        "        if len(scores_eval) > 0:\n",
        "            self.eval_reward = scores_eval[-1]\n",
        "\n",
        "    def record_summary(self, t):\n",
        "        pass\n",
        "\n",
        "    def sample_path(self, env, num_episodes=None):\n",
        "        \"\"\"\n",
        "        Sample paths (trajectories) from the environment.\n",
        "\n",
        "        Args:\n",
        "            num_episodes: the number of episodes to be sampled\n",
        "                if none, sample one batch (size indicated by config file)\n",
        "            env: open AI Gym envinronment\n",
        "\n",
        "        Returns:\n",
        "            paths: a list of paths. Each path in paths is a dictionary with\n",
        "                path[\"observation\"] a numpy array of ordered observations in the path\n",
        "                path[\"actions\"] a numpy array of the corresponding actions in the path\n",
        "                path[\"reward\"] a numpy array of the corresponding rewards in the path\n",
        "            total_rewards: the sum of all rewards encountered during this \"path\"\n",
        "\n",
        "        You do not have to implement anything in this function, but you will need to\n",
        "        understand what it returns, and it is worthwhile to look over the code\n",
        "        just so you understand how we are taking actions in the environment\n",
        "        and generating batches to train on.\n",
        "        \"\"\"\n",
        "        episode = 0\n",
        "        episode_rewards = []\n",
        "        paths = []\n",
        "        t = 0\n",
        "\n",
        "        while num_episodes or t < self.config.batch_size:\n",
        "            state = env.reset()\n",
        "            states, actions, rewards = [], [], []\n",
        "            episode_reward = 0\n",
        "\n",
        "            for step in range(self.config.max_ep_len):\n",
        "                states.append(state)\n",
        "                action = self.policy.act(states[-1][None])[0]\n",
        "                state, reward, done, info = env.step(action)\n",
        "                actions.append(action)\n",
        "                rewards.append(reward)\n",
        "                episode_reward += reward\n",
        "                t += 1\n",
        "                if done or step == self.config.max_ep_len - 1:\n",
        "                    episode_rewards.append(episode_reward)\n",
        "                    break\n",
        "                if (not num_episodes) and t == self.config.batch_size:\n",
        "                    break\n",
        "\n",
        "            path = {\n",
        "                \"observation\": np.array(states),\n",
        "                \"reward\": np.array(rewards),\n",
        "                \"action\": np.array(actions),\n",
        "            }\n",
        "            paths.append(path)\n",
        "            episode += 1\n",
        "            if num_episodes and episode >= num_episodes:\n",
        "                break\n",
        "\n",
        "        return paths, episode_rewards\n",
        "\n",
        "    def get_returns(self, paths):\n",
        "        \"\"\"\n",
        "        Calculate the returns G_t for each timestep\n",
        "\n",
        "        Args:\n",
        "            paths: recorded sample paths. See sample_path() for details.\n",
        "\n",
        "        Return:\n",
        "            returns: return G_t for each timestep\n",
        "\n",
        "        After acting in the environment, we record the observations, actions, and\n",
        "        rewards. To get the advantages that we need for the policy update, we have\n",
        "        to convert the rewards into returns, G_t, which are themselves an estimate\n",
        "        of Q^π (s_t, a_t):\n",
        "\n",
        "           G_t = r_t + γ r_{t+1} + γ^2 r_{t+2} + ... + γ^{T-t} r_T\n",
        "\n",
        "        where T is the last timestep of the episode.\n",
        "\n",
        "        Note that here we are creating a list of returns for each path\n",
        "\n",
        "        TODO: compute and return G_t for each timestep. Use self.config.gamma.\n",
        "        \"\"\"\n",
        "\n",
        "        all_returns = []\n",
        "        for path in paths:\n",
        "            rewards = path[\"reward\"]\n",
        "            #######################################################\n",
        "            #########   YOUR CODE HERE - 5-10 lines.   ############\n",
        "\n",
        "            #######################################################\n",
        "            #########          END YOUR CODE.          ############\n",
        "            all_returns.append(returns)\n",
        "        returns = np.concatenate(all_returns)\n",
        "\n",
        "        return returns\n",
        "\n",
        "    def normalize_advantage(self, advantages):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            advantages: np.array of shape [batch size]\n",
        "        Returns:\n",
        "            normalized_advantages: np.array of shape [batch size]\n",
        "\n",
        "        TODO:\n",
        "        Normalize the advantages so that they have a mean of 0 and standard\n",
        "        deviation of 1. Put the result in a variable called\n",
        "        normalized_advantages (which will be returned).\n",
        "\n",
        "        Note:\n",
        "        This function is called only if self.config.normalize_advantage is True.\n",
        "        \"\"\"\n",
        "        #######################################################\n",
        "        #########   YOUR CODE HERE - 1-2 lines.    ############\n",
        "\n",
        "        #######################################################\n",
        "        #########          END YOUR CODE.          ############\n",
        "        return normalized_advantages\n",
        "\n",
        "    def calculate_advantage(self, returns, observations):\n",
        "        \"\"\"\n",
        "        Calculates the advantage for each of the observations\n",
        "        Args:\n",
        "            returns: np.array of shape [batch size]\n",
        "            observations: np.array of shape [batch size, dim(observation space)]\n",
        "        Returns:\n",
        "            advantages: np.array of shape [batch size]\n",
        "        \"\"\"\n",
        "        advantages = returns\n",
        "\n",
        "        if self.config.normalize_advantage:\n",
        "            advantages = self.normalize_advantage(advantages)\n",
        "\n",
        "        return advantages\n",
        "\n",
        "    def update_policy(self, observations, actions, advantages):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            observations: np.array of shape [batch size, dim(observation space)]\n",
        "            actions: np.array of shape\n",
        "                [batch size, dim(action space)] if continuous\n",
        "                [batch size] (and integer type) if discrete\n",
        "            advantages: np.array of shape [batch size]\n",
        "\n",
        "        Perform one update on the policy using the provided data.\n",
        "        To compute the loss, you will need the log probabilities of the actions\n",
        "        given the observations. Note that the policy's action_distribution\n",
        "        method returns an instance of a subclass of\n",
        "        torch.distributions.Distribution, and that object can be used to\n",
        "        compute log probabilities.\n",
        "        See https://pytorch.org/docs/stable/distributions.html#distribution\n",
        "\n",
        "        Note:\n",
        "        PyTorch optimizers will try to minimize the loss you compute, but you\n",
        "        want to maximize the policy's performance.\n",
        "        \"\"\"\n",
        "        observations = np2torch(observations)\n",
        "        actions = np2torch(actions)\n",
        "        advantages = np2torch(advantages)\n",
        "        log_probs = self.policy.action_distribution(observations).log_prob(actions)\n",
        "        loss = -torch.mean(log_probs * advantages)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Performs training\n",
        "\n",
        "        You do not have to change or use anything here, but take a look\n",
        "        to see how all the code you've written fits together!\n",
        "        \"\"\"\n",
        "        last_record = 0\n",
        "\n",
        "        self.init_averages()\n",
        "        all_total_rewards = (\n",
        "            []\n",
        "        )  # the returns of all episodes samples for training purposes\n",
        "        averaged_total_rewards = []  # the returns for each iteration\n",
        "\n",
        "        for t in range(self.config.num_batches):\n",
        "\n",
        "            # collect a minibatch of samples\n",
        "            paths, total_rewards = self.sample_path(self.env)\n",
        "            all_total_rewards.extend(total_rewards)\n",
        "            observations = np.concatenate([path[\"observation\"] for path in paths])\n",
        "            actions = np.concatenate([path[\"action\"] for path in paths])\n",
        "            rewards = np.concatenate([path[\"reward\"] for path in paths])\n",
        "            # compute Q-val estimates (discounted future returns) for each time step\n",
        "            returns = self.get_returns(paths)\n",
        "\n",
        "            # advantage will depend on the baseline implementation\n",
        "            advantages = self.calculate_advantage(returns, observations)\n",
        "\n",
        "            # run training operations\n",
        "            if self.config.use_baseline:\n",
        "                self.baseline_network.update_baseline(returns, observations)\n",
        "            self.update_policy(observations, actions, advantages)\n",
        "\n",
        "            # logging\n",
        "            if t % self.config.summary_freq == 0:\n",
        "                self.update_averages(total_rewards, all_total_rewards)\n",
        "                self.record_summary(t)\n",
        "\n",
        "            # compute reward statistics for this batch and log\n",
        "            avg_reward = np.mean(total_rewards)\n",
        "            sigma_reward = np.sqrt(np.var(total_rewards) / len(total_rewards))\n",
        "            msg = \"[ITERATION {}]: Average reward: {:04.2f} +/- {:04.2f}\".format(\n",
        "                t, avg_reward, sigma_reward\n",
        "            )\n",
        "            averaged_total_rewards.append(avg_reward)\n",
        "            self.logger.info(msg)\n",
        "\n",
        "            if self.config.record and (last_record > self.config.record_freq):\n",
        "                self.logger.info(\"Recording...\")\n",
        "                last_record = 0\n",
        "                self.record()\n",
        "\n",
        "        self.logger.info(\"- Training done.\")\n",
        "        np.save(self.config.scores_output, averaged_total_rewards)\n",
        "        export_plot(\n",
        "            averaged_total_rewards,\n",
        "            \"Score\",\n",
        "            self.config.env_name,\n",
        "            self.config.plot_output,\n",
        "        )\n",
        "\n",
        "    def evaluate(self, env=None, num_episodes=1):\n",
        "        \"\"\"\n",
        "        Evaluates the return for num_episodes episodes.\n",
        "        Not used right now, all evaluation statistics are computed during training\n",
        "        episodes.\n",
        "        \"\"\"\n",
        "        if env == None:\n",
        "            env = self.env\n",
        "        paths, rewards = self.sample_path(env, num_episodes)\n",
        "        avg_reward = np.mean(rewards)\n",
        "        sigma_reward = np.sqrt(np.var(rewards) / len(rewards))\n",
        "        msg = \"Average reward: {:04.2f} +/- {:04.2f}\".format(avg_reward, sigma_reward)\n",
        "        self.logger.info(msg)\n",
        "        return avg_reward\n",
        "\n",
        "    def record(self):\n",
        "        \"\"\"\n",
        "        Recreate an env and record a video for one episode\n",
        "        \"\"\"\n",
        "        env = gym.make(self.config.env_name)\n",
        "        env.seed(self.seed)\n",
        "        env = gym.wrappers.Monitor(\n",
        "            env, self.config.record_path, video_callable=lambda x: True, resume=True\n",
        "        )\n",
        "        self.evaluate(env, 1)\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"\n",
        "        Apply procedures of training for a PG.\n",
        "        \"\"\"\n",
        "        # record one game at the beginning\n",
        "        if self.config.record:\n",
        "            self.record()\n",
        "        # model\n",
        "        self.train()\n",
        "        # record one game at the end\n",
        "        if self.config.record:\n",
        "            self.record()"
      ],
      "metadata": {
        "id": "1Ehc9L7DVCI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from config import get_config\n",
        "import gym\n",
        "\n",
        "\n",
        "seed = 123456\n",
        "config = get_config('pendulum', False, False, seed)\n",
        "config.env_name = \"InvertedPendulum-v4\"\n",
        "config.normalize_advantage = True\n",
        "env = gym.make(config.env_name)\n",
        "model = PolicyGradient(env, config, seed)\n",
        "model.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRMm14qWVCgZ",
        "outputId": "835fd3f8-db2f-4cb3-a427-28443b56ad33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n",
            "INFO:logger:[ITERATION 0]: Average reward: 8.54 +/- 0.14\n",
            "INFO:logger:[ITERATION 1]: Average reward: 12.51 +/- 0.28\n",
            "INFO:logger:[ITERATION 2]: Average reward: 20.61 +/- 0.58\n",
            "INFO:logger:[ITERATION 3]: Average reward: 27.60 +/- 0.75\n",
            "INFO:logger:[ITERATION 4]: Average reward: 37.93 +/- 1.28\n",
            "INFO:logger:[ITERATION 5]: Average reward: 46.08 +/- 1.33\n",
            "INFO:logger:[ITERATION 6]: Average reward: 51.93 +/- 1.46\n",
            "INFO:logger:[ITERATION 7]: Average reward: 60.13 +/- 1.98\n",
            "INFO:logger:[ITERATION 8]: Average reward: 66.21 +/- 1.94\n",
            "INFO:logger:[ITERATION 9]: Average reward: 73.39 +/- 2.76\n",
            "INFO:logger:[ITERATION 10]: Average reward: 75.07 +/- 2.38\n",
            "INFO:logger:[ITERATION 11]: Average reward: 69.16 +/- 2.08\n",
            "INFO:logger:[ITERATION 12]: Average reward: 84.56 +/- 3.40\n",
            "INFO:logger:[ITERATION 13]: Average reward: 111.55 +/- 5.06\n",
            "INFO:logger:[ITERATION 14]: Average reward: 160.18 +/- 9.64\n",
            "INFO:logger:[ITERATION 15]: Average reward: 203.70 +/- 11.44\n",
            "INFO:logger:[ITERATION 16]: Average reward: 202.88 +/- 6.94\n",
            "INFO:logger:[ITERATION 17]: Average reward: 231.21 +/- 10.42\n",
            "INFO:logger:[ITERATION 18]: Average reward: 311.59 +/- 19.76\n",
            "INFO:logger:[ITERATION 19]: Average reward: 217.38 +/- 13.51\n",
            "INFO:logger:[ITERATION 20]: Average reward: 218.49 +/- 14.05\n",
            "INFO:logger:[ITERATION 21]: Average reward: 269.68 +/- 21.53\n",
            "INFO:logger:[ITERATION 22]: Average reward: 207.04 +/- 17.63\n",
            "INFO:logger:[ITERATION 23]: Average reward: 243.61 +/- 19.65\n",
            "INFO:logger:[ITERATION 24]: Average reward: 332.55 +/- 26.61\n",
            "INFO:logger:[ITERATION 25]: Average reward: 358.19 +/- 20.80\n",
            "INFO:logger:[ITERATION 26]: Average reward: 301.58 +/- 11.22\n",
            "INFO:logger:[ITERATION 27]: Average reward: 332.83 +/- 13.71\n",
            "INFO:logger:[ITERATION 28]: Average reward: 419.78 +/- 36.46\n",
            "INFO:logger:[ITERATION 29]: Average reward: 498.80 +/- 33.24\n",
            "INFO:logger:[ITERATION 30]: Average reward: 462.62 +/- 31.90\n",
            "INFO:logger:[ITERATION 31]: Average reward: 596.19 +/- 41.12\n",
            "INFO:logger:[ITERATION 32]: Average reward: 719.85 +/- 55.28\n",
            "INFO:logger:[ITERATION 33]: Average reward: 911.70 +/- 44.49\n",
            "INFO:logger:[ITERATION 34]: Average reward: 1000.00 +/- 0.00\n",
            "INFO:logger:[ITERATION 35]: Average reward: 1000.00 +/- 0.00\n",
            "INFO:logger:[ITERATION 36]: Average reward: 1000.00 +/- 0.00\n",
            "INFO:logger:[ITERATION 37]: Average reward: 1000.00 +/- 0.00\n",
            "INFO:logger:[ITERATION 38]: Average reward: 1000.00 +/- 0.00\n",
            "INFO:logger:[ITERATION 39]: Average reward: 1000.00 +/- 0.00\n",
            "INFO:logger:[ITERATION 40]: Average reward: 1000.00 +/- 0.00\n",
            "INFO:logger:[ITERATION 41]: Average reward: 1000.00 +/- 0.00\n",
            "INFO:logger:[ITERATION 42]: Average reward: 342.38 +/- 9.41\n",
            "INFO:logger:[ITERATION 43]: Average reward: 799.25 +/- 57.60\n",
            "INFO:logger:[ITERATION 44]: Average reward: 1000.00 +/- 0.00\n",
            "INFO:logger:[ITERATION 45]: Average reward: 989.80 +/- 9.68\n",
            "INFO:logger:[ITERATION 46]: Average reward: 876.55 +/- 81.60\n",
            "INFO:logger:[ITERATION 47]: Average reward: 894.91 +/- 78.01\n",
            "INFO:logger:[ITERATION 48]: Average reward: 811.92 +/- 96.15\n",
            "INFO:logger:[ITERATION 49]: Average reward: 916.50 +/- 79.22\n",
            "INFO:logger:[ITERATION 50]: Average reward: 980.60 +/- 18.40\n",
            "INFO:logger:[ITERATION 51]: Average reward: 1000.00 +/- 0.00\n",
            "INFO:logger:[ITERATION 52]: Average reward: 1000.00 +/- 0.00\n",
            "INFO:logger:[ITERATION 53]: Average reward: 1000.00 +/- 0.00\n",
            "INFO:logger:[ITERATION 54]: Average reward: 1000.00 +/- 0.00\n",
            "INFO:logger:[ITERATION 55]: Average reward: 1000.00 +/- 0.00\n",
            "INFO:logger:[ITERATION 56]: Average reward: 1000.00 +/- 0.00\n",
            "INFO:logger:[ITERATION 57]: Average reward: 1000.00 +/- 0.00\n",
            "INFO:logger:[ITERATION 58]: Average reward: 1000.00 +/- 0.00\n",
            "INFO:logger:[ITERATION 59]: Average reward: 1000.00 +/- 0.00\n",
            "INFO:logger:[ITERATION 60]: Average reward: 1000.00 +/- 0.00\n",
            "INFO:logger:[ITERATION 61]: Average reward: 1000.00 +/- 0.00\n",
            "INFO:logger:[ITERATION 62]: Average reward: 903.20 +/- 91.83\n",
            "INFO:logger:[ITERATION 63]: Average reward: 872.00 +/- 88.38\n",
            "INFO:logger:[ITERATION 64]: Average reward: 1000.00 +/- 0.00\n",
            "INFO:logger:[ITERATION 65]: Average reward: 1000.00 +/- 0.00\n",
            "INFO:logger:[ITERATION 66]: Average reward: 925.10 +/- 71.06\n",
            "INFO:logger:[ITERATION 67]: Average reward: 867.45 +/- 86.39\n",
            "INFO:logger:[ITERATION 68]: Average reward: 605.47 +/- 90.36\n",
            "INFO:logger:[ITERATION 69]: Average reward: 1000.00 +/- 0.00\n",
            "INFO:logger:[ITERATION 70]: Average reward: 1000.00 +/- 0.00\n",
            "INFO:logger:[ITERATION 71]: Average reward: 1000.00 +/- 0.00\n",
            "INFO:logger:[ITERATION 72]: Average reward: 1000.00 +/- 0.00\n",
            "INFO:logger:[ITERATION 73]: Average reward: 1000.00 +/- 0.00\n",
            "INFO:logger:[ITERATION 74]: Average reward: 1000.00 +/- 0.00\n",
            "INFO:logger:[ITERATION 75]: Average reward: 1000.00 +/- 0.00\n",
            "INFO:logger:[ITERATION 76]: Average reward: 1000.00 +/- 0.00\n",
            "INFO:logger:[ITERATION 77]: Average reward: 1000.00 +/- 0.00\n",
            "INFO:logger:[ITERATION 78]: Average reward: 1000.00 +/- 0.00\n",
            "INFO:logger:[ITERATION 79]: Average reward: 1000.00 +/- 0.00\n",
            "INFO:logger:[ITERATION 80]: Average reward: 1000.00 +/- 0.00\n",
            "INFO:logger:[ITERATION 81]: Average reward: 1000.00 +/- 0.00\n",
            "INFO:logger:[ITERATION 82]: Average reward: 1000.00 +/- 0.00\n",
            "INFO:logger:[ITERATION 83]: Average reward: 1000.00 +/- 0.00\n",
            "INFO:logger:[ITERATION 84]: Average reward: 1000.00 +/- 0.00\n",
            "INFO:logger:[ITERATION 85]: Average reward: 1000.00 +/- 0.00\n",
            "INFO:logger:[ITERATION 86]: Average reward: 1000.00 +/- 0.00\n",
            "INFO:logger:[ITERATION 87]: Average reward: 1000.00 +/- 0.00\n",
            "INFO:logger:[ITERATION 88]: Average reward: 1000.00 +/- 0.00\n",
            "INFO:logger:[ITERATION 89]: Average reward: 1000.00 +/- 0.00\n",
            "INFO:logger:[ITERATION 90]: Average reward: 1000.00 +/- 0.00\n",
            "INFO:logger:[ITERATION 91]: Average reward: 1000.00 +/- 0.00\n",
            "INFO:logger:[ITERATION 92]: Average reward: 1000.00 +/- 0.00\n",
            "INFO:logger:[ITERATION 93]: Average reward: 902.60 +/- 92.40\n",
            "INFO:logger:[ITERATION 94]: Average reward: 1000.00 +/- 0.00\n",
            "INFO:logger:[ITERATION 95]: Average reward: 900.80 +/- 94.11\n",
            "INFO:logger:[ITERATION 96]: Average reward: 1000.00 +/- 0.00\n",
            "INFO:logger:[ITERATION 97]: Average reward: 1000.00 +/- 0.00\n",
            "INFO:logger:[ITERATION 98]: Average reward: 900.50 +/- 94.39\n",
            "INFO:logger:[ITERATION 99]: Average reward: 1000.00 +/- 0.00\n",
            "INFO:logger:- Training done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zadanie\n",
        "\n",
        "Aby uruchomić kod skopiuj do środowiska colab pliki dołączone do zadania.\n",
        "Wykresy znajdziesz w folderze `./results`\n",
        "\n",
        "1. Powyższa implementacja wykorzystuje `GaussianPolicy`. Wytłumacz na czym polega próbkowanie akcji ciągłych z wykorzystaniem `GaussianPolicy`. Pokaż przykład spróbkowanej akcji w środowisku `InvertedPendulum`.\n",
        "2. Uzupełnij brakujący kod z metod `get_returns` (wykorzystaj `self.config.gamma`, czyli współczynnik dyskontowania) oraz `normalize_advantage`.\n",
        "3. Porównaj wykresy \"score do iteracji\" z i bez normalizacji. Co zauważasz?"
      ],
      "metadata": {
        "id": "qE6BYBLRq5BS"
      }
    }
  ]
}