{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Policy Gradients\n",
        "\n",
        "\n",
        "Policy gradient to metoda uczenia przez wzmacnianie, która polega na **bezpośrednim uczeniu strategii**, czyli funkcji $ \\pi_\\theta(a|s) $, która określa prawdopodobieństwo wyboru akcji $ a $ w stanie $ s $. Zamiast optymalizować funkcję wartości (jak w Q-learning), policy gradient optymalizuje samą strategię, tak aby maksymalizować długoterminową oczekiwaną nagrodę $ J(\\pi_\\theta) $.\n",
        "\n",
        "**Podstawowe kroki policy gradient**:\n",
        "\n",
        "1. **Próbkowanie doświadczeń**:\n",
        "- Agent działa w środowisku zgodnie z aktualną strategią, zbierając trajektorie (stany, akcje, nagrody).\n",
        "\n",
        "2. **Obliczanie gradientu**:\n",
        "- Funkcja celu (np. REINFORCE) opiera się na policy gradient theorem:\n",
        "  $$\n",
        "  \\nabla_\\theta J(\\pi_\\theta) = \\mathbb{E}[\\nabla_\\theta \\log \\pi_\\theta(a|s) \\cdot R(\\tau)]\n",
        "  $$\n",
        "  - $ \\nabla_\\theta \\log \\pi_\\theta(a|s) $: Jak zmienić parametry $ \\theta $, aby zwiększyć prawdopodobieństwo akcji $ a $ w stanie $ s $.\n",
        "  - $ R(\\tau) $: Nagroda z całej trajektorii, używana do oceny, jak \"dobry\" był wybór akcji.\n",
        "\n",
        "3. **Aktualizacja strategii**:\n",
        "- Parametry $ \\theta $ są aktualizowane w kierunku, który zwiększa prawdopodobieństwo akcji prowadzących do wysokich nagród.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uqTL6cmo3fq9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Poniżej znajduje się najprostsza implementacja *policy gradients* na przykładzie środowiska `CartPole-v0`. Osiąga ona zwrot maksymalny 195. [Leaderboard](https://github.com/openai/gym/wiki/Leaderboard#cartpole-v0) wynosi $382$). Poniższa implementacja osiąga ~194, co możemy uznać za rozwiązanie problemu `CartPole`."
      ],
      "metadata": {
        "id": "lD90HhBiiXqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions.categorical import Categorical\n",
        "from torch.optim import Adam\n",
        "import numpy as np\n",
        "import gym\n",
        "from gym.spaces import Discrete, Box\n",
        "\n",
        "def mlp(sizes, activation=nn.Tanh, output_activation=nn.Identity):\n",
        "    # Build a feedforward neural network.\n",
        "    layers = []\n",
        "    for j in range(len(sizes)-1):\n",
        "        act = activation if j < len(sizes)-2 else output_activation\n",
        "        layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "def train(env_name='CartPole-v0', hidden_sizes=[32], lr=1e-2,\n",
        "          epochs=100, batch_size=5000, render=False):\n",
        "\n",
        "    # make environment, check spaces, get obs / act dims\n",
        "    env = gym.make(env_name)\n",
        "    assert isinstance(env.observation_space, Box), \\\n",
        "        \"This example only works for envs with continuous state spaces.\"\n",
        "    assert isinstance(env.action_space, Discrete), \\\n",
        "        \"This example only works for envs with discrete action spaces.\"\n",
        "\n",
        "    obs_dim = env.observation_space.shape[0]\n",
        "    n_acts = env.action_space.n\n",
        "\n",
        "    # make core of policy network\n",
        "    logits_net = mlp(sizes=[obs_dim]+hidden_sizes+[n_acts])\n",
        "\n",
        "    # make function to compute action distribution\n",
        "    def get_policy(obs):\n",
        "        logits = logits_net(obs)\n",
        "        return Categorical(logits=logits)\n",
        "\n",
        "    # make action selection function (outputs int actions, sampled from policy)\n",
        "    def get_action(obs):\n",
        "        return get_policy(obs).sample().item()\n",
        "\n",
        "    # make loss function whose gradient, for the right data, is policy gradient\n",
        "    def compute_loss(obs, act, weights):\n",
        "        logp = get_policy(obs).log_prob(act)\n",
        "        return -(logp * weights).mean()\n",
        "\n",
        "    # make optimizer\n",
        "    optimizer = Adam(logits_net.parameters(), lr=lr)\n",
        "\n",
        "    # for training policy\n",
        "    def train_one_epoch():\n",
        "        # make some empty lists for logging.\n",
        "        batch_obs = []          # for observations\n",
        "        batch_acts = []         # for actions\n",
        "        batch_weights = []      # for R(tau) weighting in policy gradient\n",
        "        batch_rets = []         # for measuring episode returns\n",
        "        batch_lens = []         # for measuring episode lengths\n",
        "\n",
        "        # reset episode-specific variables\n",
        "        obs = env.reset()       # first obs comes from starting distribution\n",
        "        done = False            # signal from environment that episode is over\n",
        "        ep_rews = []            # list for rewards accrued throughout ep\n",
        "\n",
        "        # render first episode of each epoch\n",
        "        finished_rendering_this_epoch = False\n",
        "\n",
        "        # collect experience by acting in the environment with current policy\n",
        "        while True:\n",
        "\n",
        "            # rendering\n",
        "            if (not finished_rendering_this_epoch) and render:\n",
        "                env.render()\n",
        "\n",
        "            # save obs\n",
        "            batch_obs.append(obs.copy())\n",
        "\n",
        "            # act in the environment\n",
        "            act = get_action(torch.as_tensor(obs, dtype=torch.float32))\n",
        "            obs, rew, done, _ = env.step(act)\n",
        "\n",
        "            # save action, reward\n",
        "            batch_acts.append(act)\n",
        "            ep_rews.append(rew)\n",
        "\n",
        "            if done:\n",
        "                # if episode is over, record info about episode\n",
        "                ep_ret, ep_len = sum(ep_rews), len(ep_rews)\n",
        "                batch_rets.append(ep_ret)\n",
        "                batch_lens.append(ep_len)\n",
        "\n",
        "                # the weight for each logprob(a|s) is R(tau)\n",
        "                batch_weights += [ep_ret] * ep_len\n",
        "\n",
        "                # reset episode-specific variables\n",
        "                obs, done, ep_rews = env.reset(), False, []\n",
        "\n",
        "                # won't render again this epoch\n",
        "                finished_rendering_this_epoch = True\n",
        "\n",
        "                # end experience loop if we have enough of it\n",
        "                if len(batch_obs) > batch_size:\n",
        "                    break\n",
        "\n",
        "        # take a single policy gradient update step\n",
        "        optimizer.zero_grad()\n",
        "        batch_loss = compute_loss(obs=torch.as_tensor(batch_obs, dtype=torch.float32),\n",
        "                                  act=torch.as_tensor(batch_acts, dtype=torch.int32),\n",
        "                                  weights=torch.as_tensor(batch_weights, dtype=torch.float32)\n",
        "                                  )\n",
        "        batch_loss.backward()\n",
        "        optimizer.step()\n",
        "        return batch_loss, batch_rets, batch_lens\n",
        "\n",
        "    # training loop\n",
        "    for i in range(epochs):\n",
        "        batch_loss, batch_rets, batch_lens = train_one_epoch()\n",
        "        print('epoch: %3d \\t loss: %.3f \\t return: %.3f \\t ep_len: %.3f'%\n",
        "                (i, batch_loss, np.mean(batch_rets), np.mean(batch_lens)))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print('\\nUsing simplest formulation of policy gradient.\\n')\n",
        "    train(env_name='CartPole-v0', render=True, lr=1e-2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHMOMRyvzZ38",
        "outputId": "f77c568c-b76e-44a7-baff-ca51cbfa38a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Using simplest formulation of policy gradient.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:   0 \t loss: 15.526 \t return: 18.281 \t ep_len: 18.281\n",
            "epoch:   1 \t loss: 18.902 \t return: 21.496 \t ep_len: 21.496\n",
            "epoch:   2 \t loss: 21.356 \t return: 23.952 \t ep_len: 23.952\n",
            "epoch:   3 \t loss: 24.615 \t return: 27.388 \t ep_len: 27.388\n",
            "epoch:   4 \t loss: 29.083 \t return: 33.007 \t ep_len: 33.007\n",
            "epoch:   5 \t loss: 29.323 \t return: 34.397 \t ep_len: 34.397\n",
            "epoch:   6 \t loss: 29.674 \t return: 36.326 \t ep_len: 36.326\n",
            "epoch:   7 \t loss: 31.892 \t return: 38.313 \t ep_len: 38.313\n",
            "epoch:   8 \t loss: 28.470 \t return: 37.228 \t ep_len: 37.228\n",
            "epoch:   9 \t loss: 34.139 \t return: 42.863 \t ep_len: 42.863\n",
            "epoch:  10 \t loss: 31.716 \t return: 42.390 \t ep_len: 42.390\n",
            "epoch:  11 \t loss: 31.855 \t return: 44.652 \t ep_len: 44.652\n",
            "epoch:  12 \t loss: 36.741 \t return: 50.050 \t ep_len: 50.050\n",
            "epoch:  13 \t loss: 38.042 \t return: 51.670 \t ep_len: 51.670\n",
            "epoch:  14 \t loss: 33.503 \t return: 48.163 \t ep_len: 48.163\n",
            "epoch:  15 \t loss: 40.256 \t return: 57.330 \t ep_len: 57.330\n",
            "epoch:  16 \t loss: 34.951 \t return: 52.365 \t ep_len: 52.365\n",
            "epoch:  17 \t loss: 40.107 \t return: 56.382 \t ep_len: 56.382\n",
            "epoch:  18 \t loss: 42.488 \t return: 62.099 \t ep_len: 62.099\n",
            "epoch:  19 \t loss: 35.596 \t return: 54.783 \t ep_len: 54.783\n",
            "epoch:  20 \t loss: 35.704 \t return: 55.143 \t ep_len: 55.143\n",
            "epoch:  21 \t loss: 39.119 \t return: 59.400 \t ep_len: 59.400\n",
            "epoch:  22 \t loss: 38.507 \t return: 58.744 \t ep_len: 58.744\n",
            "epoch:  23 \t loss: 42.421 \t return: 66.105 \t ep_len: 66.105\n",
            "epoch:  24 \t loss: 49.172 \t return: 68.753 \t ep_len: 68.753\n",
            "epoch:  25 \t loss: 45.918 \t return: 68.945 \t ep_len: 68.945\n",
            "epoch:  26 \t loss: 47.265 \t return: 72.507 \t ep_len: 72.507\n",
            "epoch:  27 \t loss: 44.118 \t return: 70.718 \t ep_len: 70.718\n",
            "epoch:  28 \t loss: 48.550 \t return: 77.462 \t ep_len: 77.462\n",
            "epoch:  29 \t loss: 50.996 \t return: 80.222 \t ep_len: 80.222\n",
            "epoch:  30 \t loss: 56.802 \t return: 88.667 \t ep_len: 88.667\n",
            "epoch:  31 \t loss: 56.841 \t return: 87.860 \t ep_len: 87.860\n",
            "epoch:  32 \t loss: 54.802 \t return: 85.559 \t ep_len: 85.559\n",
            "epoch:  33 \t loss: 57.929 \t return: 91.218 \t ep_len: 91.218\n",
            "epoch:  34 \t loss: 61.903 \t return: 97.192 \t ep_len: 97.192\n",
            "epoch:  35 \t loss: 68.384 \t return: 108.106 \t ep_len: 108.106\n",
            "epoch:  36 \t loss: 70.697 \t return: 116.465 \t ep_len: 116.465\n",
            "epoch:  37 \t loss: 75.378 \t return: 124.049 \t ep_len: 124.049\n",
            "epoch:  38 \t loss: 77.878 \t return: 125.725 \t ep_len: 125.725\n",
            "epoch:  39 \t loss: 81.575 \t return: 133.500 \t ep_len: 133.500\n",
            "epoch:  40 \t loss: 94.438 \t return: 164.452 \t ep_len: 164.452\n",
            "epoch:  41 \t loss: 100.736 \t return: 179.893 \t ep_len: 179.893\n",
            "epoch:  42 \t loss: 101.861 \t return: 180.893 \t ep_len: 180.893\n",
            "epoch:  43 \t loss: 105.076 \t return: 190.074 \t ep_len: 190.074\n",
            "epoch:  44 \t loss: 102.966 \t return: 187.593 \t ep_len: 187.593\n",
            "epoch:  45 \t loss: 103.524 \t return: 186.407 \t ep_len: 186.407\n",
            "epoch:  46 \t loss: 106.158 \t return: 192.885 \t ep_len: 192.885\n",
            "epoch:  47 \t loss: 107.284 \t return: 197.923 \t ep_len: 197.923\n",
            "epoch:  48 \t loss: 103.269 \t return: 182.071 \t ep_len: 182.071\n",
            "epoch:  49 \t loss: 101.273 \t return: 181.643 \t ep_len: 181.643\n",
            "epoch:  50 \t loss: 105.652 \t return: 193.462 \t ep_len: 193.462\n",
            "epoch:  51 \t loss: 102.105 \t return: 177.690 \t ep_len: 177.690\n",
            "epoch:  52 \t loss: 107.237 \t return: 195.615 \t ep_len: 195.615\n",
            "epoch:  53 \t loss: 102.530 \t return: 185.000 \t ep_len: 185.000\n",
            "epoch:  54 \t loss: 105.231 \t return: 192.962 \t ep_len: 192.962\n",
            "epoch:  55 \t loss: 102.956 \t return: 185.714 \t ep_len: 185.714\n",
            "epoch:  56 \t loss: 104.345 \t return: 193.808 \t ep_len: 193.808\n",
            "epoch:  57 \t loss: 102.252 \t return: 193.308 \t ep_len: 193.308\n",
            "epoch:  58 \t loss: 104.550 \t return: 197.769 \t ep_len: 197.769\n",
            "epoch:  59 \t loss: 99.685 \t return: 183.607 \t ep_len: 183.607\n",
            "epoch:  60 \t loss: 101.604 \t return: 192.296 \t ep_len: 192.296\n",
            "epoch:  61 \t loss: 102.981 \t return: 186.444 \t ep_len: 186.444\n",
            "epoch:  62 \t loss: 104.286 \t return: 195.615 \t ep_len: 195.615\n",
            "epoch:  63 \t loss: 106.175 \t return: 200.000 \t ep_len: 200.000\n",
            "epoch:  64 \t loss: 104.200 \t return: 196.615 \t ep_len: 196.615\n",
            "epoch:  65 \t loss: 104.166 \t return: 191.556 \t ep_len: 191.556\n",
            "epoch:  66 \t loss: 107.201 \t return: 196.885 \t ep_len: 196.885\n",
            "epoch:  67 \t loss: 106.739 \t return: 194.308 \t ep_len: 194.308\n",
            "epoch:  68 \t loss: 106.743 \t return: 194.385 \t ep_len: 194.385\n",
            "epoch:  69 \t loss: 106.615 \t return: 195.154 \t ep_len: 195.154\n",
            "epoch:  70 \t loss: 108.000 \t return: 197.346 \t ep_len: 197.346\n",
            "epoch:  71 \t loss: 105.942 \t return: 185.741 \t ep_len: 185.741\n",
            "epoch:  72 \t loss: 108.719 \t return: 197.308 \t ep_len: 197.308\n",
            "epoch:  73 \t loss: 106.284 \t return: 187.963 \t ep_len: 187.963\n",
            "epoch:  74 \t loss: 104.723 \t return: 186.296 \t ep_len: 186.296\n",
            "epoch:  75 \t loss: 107.228 \t return: 192.185 \t ep_len: 192.185\n",
            "epoch:  76 \t loss: 107.701 \t return: 191.519 \t ep_len: 191.519\n",
            "epoch:  77 \t loss: 108.442 \t return: 191.519 \t ep_len: 191.519\n",
            "epoch:  78 \t loss: 103.085 \t return: 179.250 \t ep_len: 179.250\n",
            "epoch:  79 \t loss: 104.653 \t return: 178.931 \t ep_len: 178.931\n",
            "epoch:  80 \t loss: 104.216 \t return: 173.207 \t ep_len: 173.207\n",
            "epoch:  81 \t loss: 107.815 \t return: 185.393 \t ep_len: 185.393\n",
            "epoch:  82 \t loss: 106.245 \t return: 180.750 \t ep_len: 180.750\n",
            "epoch:  83 \t loss: 106.304 \t return: 173.793 \t ep_len: 173.793\n",
            "epoch:  84 \t loss: 106.748 \t return: 182.821 \t ep_len: 182.821\n",
            "epoch:  85 \t loss: 105.650 \t return: 173.267 \t ep_len: 173.267\n",
            "epoch:  86 \t loss: 107.542 \t return: 168.667 \t ep_len: 168.667\n",
            "epoch:  87 \t loss: 107.569 \t return: 179.000 \t ep_len: 179.000\n",
            "epoch:  88 \t loss: 106.316 \t return: 169.200 \t ep_len: 169.200\n",
            "epoch:  89 \t loss: 109.241 \t return: 176.828 \t ep_len: 176.828\n",
            "epoch:  90 \t loss: 109.997 \t return: 182.750 \t ep_len: 182.750\n",
            "epoch:  91 \t loss: 113.071 \t return: 188.037 \t ep_len: 188.037\n",
            "epoch:  92 \t loss: 108.781 \t return: 181.000 \t ep_len: 181.000\n",
            "epoch:  93 \t loss: 110.379 \t return: 182.893 \t ep_len: 182.893\n",
            "epoch:  94 \t loss: 110.148 \t return: 177.724 \t ep_len: 177.724\n",
            "epoch:  95 \t loss: 113.145 \t return: 194.231 \t ep_len: 194.231\n",
            "epoch:  96 \t loss: 113.582 \t return: 195.231 \t ep_len: 195.231\n",
            "epoch:  97 \t loss: 113.641 \t return: 199.000 \t ep_len: 199.000\n",
            "epoch:  98 \t loss: 111.670 \t return: 193.269 \t ep_len: 193.269\n",
            "epoch:  99 \t loss: 111.730 \t return: 194.692 \t ep_len: 194.692\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementacja strategii"
      ],
      "metadata": {
        "id": "MxPAaQ_cEi5H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sieć neuronowa strategii\n",
        "\n",
        "```python\n",
        "# make core of policy network\n",
        "logits_net = mlp(sizes=[obs_dim]+hidden_sizes+[n_acts])\n",
        "\n",
        "# make function to compute action distribution\n",
        "def get_policy(obs):\n",
        "    logits = logits_net(obs)\n",
        "    return Categorical(logits=logits)\n",
        "\n",
        "# make action selection function (outputs int actions, sampled from policy)\n",
        "def get_action(obs):\n",
        "    return get_policy(obs).sample().item()\n",
        "```"
      ],
      "metadata": {
        "id": "DBr4RE72GXq2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ten blok buduje najprostrzą sieć neuronową typu perceptron dla strategii *kategorycznej* (czyli takiej, która ma dyskretną liczbę akcji).  \n",
        "  \n",
        "Wyjściem z `logits_net` są logity, czyli logarytmy prawdopodobieństw akcji. Funkcja `get_action` próbkuje akcję na podstawie prawdopodobieństw obliczonych z logitów.  \n",
        "\n",
        "```\n",
        "Sieć zwraca logity (log prawdopodobieństwa), ponieważ:\n",
        "\n",
        "- są stabilniejsze numerycznie.\n",
        "- ułatwiają obliczenia z logarytmem prawdopodobieństw.\n",
        "- mogą być łatwo przekształcone w prawdopodobieństwa za pomocą Softmax.\n",
        "- dają sieci więcej swobody w nauce.\n",
        "\n",
        "To standardowe podejście w modelach, które pracują z kategoriami i rozkładami prawdopodobieństwa.\n",
        "```\n",
        "\n",
        "(**Uwaga**: ta konkretna funkcja `get_action` zakłada, że zostanie dostarczona tylko jedna obserwacja `obs`, a zatem tylko jedna liczba całkowita reprezentująca akcję. Dlatego używa `.item()`, który pobiera zawartość tensora zawierającego tylko jeden element).\n",
        "\n",
        "Duża część pracy w tym przykładzie jest wykonywana przez obiekt `Categorical`.  \n",
        "Jest to obiekt z PyTorch, który obejmuje szereg funkcji matematycznych związanych z rozkładami prawdopodobieństwa.  \n",
        "W szczególności zawiera metodę do próbkowania z rozkładu (którą używamy na linii 40) oraz metodę do obliczania log-prawdopodobieństw dla danych próbek (które użyjemy później).  \n",
        "Ponieważ rozkłady w PyTorch są bardzo przydatne dla RL, sprawdź ich [dokumentację](https://pytorch.org/docs/stable/distributions.html), aby lepiej zrozumieć, jak działają.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tNuVeT-rHlIy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Struktura sieci\n",
        "\n",
        "Sieć neuronowa zdefiniowana w tym kodzie to **perceptron wielowarstwowy (MLP)** zbudowany za pomocą funkcji `mlp`:\n",
        "\n",
        "1. **Warstwa wejściowa**:\n",
        "   - Rozmiar: Liczba cech w przestrzeni obserwacji środowiska (`obs_dim`).\n",
        "   - Przykład (dla `CartPole-v0`): Rozmiar wejściowy = 4 (pozycja wózka, prędkość wózka, kąt kija, prędkość kątowa kija).\n",
        "   ```python\n",
        "   obs = [0.0169022, -0.00525023, 0.0194337, 0.0132078]\n",
        "   ```\n",
        "\n",
        "2. **Warstwy ukryte**:\n",
        "   - Konfigurowane za pomocą parametru `hidden_sizes`. W tym przykładzie jest jedna warstwa ukryta z 32 neuronami, z funkcją aktywacji Tanh.\n",
        "\n",
        "3. **Warstwa wyjściowa**:\n",
        "   - Rozmiar: Liczba dostępnych akcji w środowisku (`n_acts`).\n",
        "   - Przykład (dla `CartPole-v0`): Rozmiar wyjściowy = 2 (lewo lub prawo).\n",
        "   ```python\n",
        "   logits = [1.5, -0.8]\n",
        "   ```\n",
        "   Są to nieznormalizowane wartości dla każdej akcji (lewo i prawo). Normalizacja jest dokonywana w [Categorical](https://pytorch.org/docs/stable/distributions.html#categorical) przy pomocy funkcji [Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html). Inaczej mówiąc, Logity są przekształcane w prawdopodobieństwa za pomocą funkcji softmax.\n"
      ],
      "metadata": {
        "id": "DAN_suCpLV-n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Funkcja straty\n",
        "\n",
        "**Celem jest dostosowanie parametrów strategii $ \\theta $ w kierunku, który zwiększa prawdopodobieństwo działań prowadzących do wyższych nagród.**\n",
        "\n",
        "```python\n",
        "# make loss function whose gradient, for the right data, is policy gradient\n",
        "def compute_loss(obs, act, weights):\n",
        "    logp = get_policy(obs).log_prob(act)\n",
        "    return -(logp * weights).mean()\n",
        "```\n"
      ],
      "metadata": {
        "id": "LdZQJFuyGptf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wejściem do funkcji \"straty\" (właściwie celu) jest \"batch\" trajektorii, na który składają się obserwacje, akcje oraz suma nagród uzyskana dla danej trajektorii. Przykład (pojedynczego elementu z batcha):\n",
        "```\n",
        "obs = [[0.5, 1.2], [0.3, -0.7]]\n",
        "act = [0, 1]\n",
        "weights = [50, 50]\n",
        "```\n",
        "\n",
        "Każda logarytmiczna wartość prawdopodobieństwa jest skalowana odpowiednią wagą (w najprostszej formie algorytmu gradientu strategii, stosowanej tutaj, ta sama wartość zwrotu jest przypisywana wszystkim działaniom w ramach jednej trajektorii).  \n",
        "To **ważenie** zapewnia aktualizację gradientu w celu zwiększania prawdopodobieństwa działań, które przyniosły wyższe nagrody.  \n",
        "**Średnia** zapewnia, że funkcja straty reprezentuje średnią wydajność dla wszystkich trajektorii w batchu.  \n",
        "Dzięki **odwróceniu znaku** funkcji straty, optymalizator zwiększa prawdopodobieństwo działań przynoszących większe nagrody (większość optymalizatorów, np. Adam, minimalizuje funkcję straty, ale w tym przypadku chcemy maksymalizować cel).\n",
        "\n",
        "**Intuicja:**  \n",
        "- Działania z wyższymi zwrotami (nagrodami) mają większe wagi, więc ich logarytmiczne prawdopodobieństwa są bardziej zwiększane.  \n",
        "- Działania z niższymi zwrotami mają mniejsze wagi (lub ujemne wagi w przypadku kar), więc ich logarytmiczne prawdopodobieństwa są zmniejszane."
      ],
      "metadata": {
        "id": "l_LFa0-kUTIo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pętla trenująca\n",
        "\n",
        "1. **Zbieranie doświadczeń**:\n",
        "   - Agent działa w środowisku, wykonując akcje oparte na bieżącej polityce (`get_action()`).\n",
        "   - Obserwacje, wykonane akcje i otrzymane nagrody są zapisywane.\n",
        "\n",
        "2. **Koniec epizodu**:\n",
        "   - Po zakończeniu epizodu (sygnał `done`) sumowana jest całkowita nagroda $ R(\\tau) $ oraz długość epizodu.\n",
        "   - Wagi (`weights`) są ustawiane jako powtórzona wartość $ R(\\tau) $ dla każdego kroku epizodu.\n",
        "\n",
        "3. **Warunek zakończenia**:\n",
        "   - Jeśli liczba zebranych obserwacji przekracza `batch_size`, kończy się zbieranie danych w tej epoce.\n",
        "\n",
        "4. **Aktualizacja polityki**:\n",
        "   - Funkcja straty (`compute_loss`) oblicza gradient na podstawie zebranych danych.\n",
        "   - Optymalizator aktualizuje parametry sieci neuronowej na podstawie obliczonego gradientu.\n",
        "\n",
        "\n",
        "Pętla ta powtarza się przez określoną liczbę epok (`epochs`), zbierając doświadczenia, aktualizując politykę i poprawiając działania agenta w środowisku."
      ],
      "metadata": {
        "id": "jcGHfhR4HDdY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zadanie\n",
        "\n",
        "1. Pokaż wykres **zwrotu w kolejnych epokach**: ilustruje poprawę w sumie nagród zdobytych w trajektorii, co jest bezpośrednią miarą wydajności agenta.\n",
        "2. Zaimplementuj **baseline**. W metodach policy gradient baseline to element odejmowany od nagrody (lub zwrotu, $R(\\tau)$), który ma na celu zmniejszenie wariancji oszacowań gradientu. Dzięki temu proces uczenia staje się bardziej stabilny i szybszy.\n",
        "\n",
        "Prostym i skutecznym **baseline** jest średnia zwrotów zebranych w bieżym batchu. Zamiast bezpośrednio używać zwrotów $R(\\tau)$ w *loss-function*, odejmujemy od nich ich średnią $mean(R(\\tau))$.\n",
        "\n",
        "\n",
        "1. **Oblicz baseline**: Wyznacz średnią wszystkich zwrotów w batchu:\n",
        "   $$\n",
        "   b = \\frac{1}{N} \\sum_{i=1}^N R_i\n",
        "   $$\n",
        "\n",
        "   gdzie $N$ to liczba epizodów w batchu.\n",
        "\n",
        "2. **Odejmij baseline**: Dostosuj wagi używane w obliczeniach gradientu:\n",
        "   $$\n",
        "   \\text{Adjusted weight} = R_i - b\n",
        "   $$\n",
        "\n",
        "3. **Zaktualizuj funkcję straty**: Zmodyfikuj funkcję `compute_loss`, aby uwzględnić baseline.\n",
        "\n",
        "Porównaj wykresy *zwrotu w kolejnych epokach* dla algorytmu z i bez *baseline*"
      ],
      "metadata": {
        "id": "vaXbeAOLifMg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Materiały\n",
        "\n",
        "1. [Deep RL Bootcamp Lecture 4A: Policy Gradients](https://www.youtube.com/watch?v=S_gwYj1Q-44)\n",
        "2. [Deep RL Bootcamp Lecture 4B Policy Gradients Revisited\n",
        "](https://www.youtube.com/watch?v=tqrcjHuNdmQ)\n",
        "3. [SpinningUp OpenAI Intro to PO](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html)\n",
        "4. [The spelled-out intro to neural networks and backpropagation: building micrograd](https://www.youtube.com/watch?v=VMj-3S1tku0&t)"
      ],
      "metadata": {
        "id": "EcR5VCZ1ihbf"
      }
    }
  ]
}